{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import SimpleRNN, Embedding, Dense, LSTM, Activation, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "f = open('./glove.6B.100d.txt', encoding=\"utf8\") # Open file\n",
    "\n",
    "# In the dataset, each line represents a new word embedding\n",
    "# The line starts with the word and the embedding values follow\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "    embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "    \n",
    "    = embedding # Add embedding to our embedding dictionary\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./SPAMData.csv\")\n",
    "texts = []\n",
    "labels = []\n",
    "for i, label in enumerate(data['Category']):\n",
    "    texts.append(data['Message'][i])\n",
    "    if label == 'ham':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "texts = np.asarray(texts)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "max_length = 100\n",
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (5572, 100)\n",
      "Shape of labels: (5572, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of all embeddings\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean() # Calculate mean\n",
    "emb_std = all_embs.std() # Calculate standard deviation\n",
    "\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "# Create a random matrix with the same mean and std as the embeddings\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "#print(len(word_index))\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i > vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    #print(i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 100, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 100, 128)          117248    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10,166,786\n",
      "Trainable params: 166,786\n",
      "Non-trainable params: 10,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable = False))\n",
    "\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/2\n",
      "4457/4457 [==============================] - 62s 14ms/step - loss: 0.1657 - acc: 0.9414 - val_loss: 0.0804 - val_acc: 0.9762\n",
      "Epoch 2/2\n",
      "4457/4457 [==============================] - 59s 13ms/step - loss: 0.0839 - acc: 0.9712 - val_loss: 0.0670 - val_acc: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2187e1275c0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(data,labels,validation_split=0.2,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  719   72    4  842  440  236    3   17  109  441\n",
      "    2 2998 1330  154  962    2  129   16 2999  129  414 3000  516  963\n",
      "  581   65]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0  689   73    4  803  434  234    3   17  113  435\n",
      "     2 2341 1208  154  907    2  130   16 2342  130  396 2343  505  908\n",
      "   560   65]]\n",
      "[[0.03245144 0.9576627 ]]\n"
     ]
    }
   ],
   "source": [
    "guess = [\"WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"]\n",
    "guess = np.asarray(guess[:100])\n",
    "#guess = tokenizer.fit_on_texts(guess)\n",
    "guess = tokenizer.texts_to_sequences(guess)\n",
    "#guess = pad_sequences(guess, maxlen=100)\n",
    "#guess = tokenizer.texts_to_sequences_generator\n",
    "\n",
    "#print(guess)\n",
    "print(data[8])\n",
    "\n",
    "\n",
    "predict = model.predict(guess)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
